# Rent prices

## Descripci√≥n üöÄ

Repositorio realizado para la pr√°ctica 1 de web scraping de la asignatura _Tipolog√≠a y ciclo de vida de los datos_ del M√°ster en Ciencia de Datos de la [Universitat Oberta de Catalunya](https://www.uoc.edu/portal/en/index.html).

## Contenido üì¶



## Autores ‚úíÔ∏è

* Jose Ventura Roda
* Kevin Mart√≠n Chinea

## Recursos üìÑ

## Sobre estructura del proyecto

He creado una peque√±a estructura para intentar organizar todo el c√≥digo:

```
rent_prices/
‚îú‚îÄ‚îÄ docker-compose.yml    --> Configuraci√≥n selenium y scraper
‚îú‚îÄ‚îÄ Dockerfile            --> Imagen scraper
‚îú‚îÄ‚îÄ logfile.log           --> Fichero de log
‚îú‚îÄ‚îÄ pdf
‚îÇ   ‚îú‚îÄ‚îÄ images
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ src
‚îÇ   ‚îú‚îÄ‚îÄ rent_prices
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logger
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logger.py           --> logger de la app
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logging_config.ini  --> Configuraci√≥n logger
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __pycache__
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scrapers
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scraperMock.py      --> Ejemplo scraper
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ scraper.py          --> Clase base  
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ testPackApp.py      --> Ejemplo lanzador app.
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utilities
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ proxyManager.py     --> Pruebas proxies rotatorios
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ __pycache__
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt    --> Dependencias python
‚îî‚îÄ‚îÄ tests
    ‚îî‚îÄ‚îÄ __init__.py

```

## Sobre proxies rotatorios

He probado los proxies de "http://free-proxy-list.net" y en la primera prueba me ha indicado que funcionaba s√≥lo 1 de la lista, pero al probar por segunda vez ya me ha indicado que no funcionaba ninguno.


## Sobre arranque aplicaci√≥n en docker

para arrancar la aplicaci√≥n hay que ejecutar el comando:

```
docker-compose up --build --abort-on-container-exit 
```

Esta instrucci√≥n reconstruye el docker e inicia el servicio de Selenium y del Scraper para iniciar la extracci√≥n de informaci√≥n. El par√°metro `--abort-on-container-exit` hace que cuando un contenedor se detiene se pare el resto de contenedores, as√≠ cuando el Scraper finaliza de extraer la informaci√≥n se para tambi√©n el contenedor de selenium.

El objetivo de llevar el desarrollo a contenedores es poder desplegarlo en un servidor de forma sencilla. Una vez que se finalice la programaci√≥n de la aplicaci√≥n, se puede construir una imagen y crear un contenedor a partir de la imagen.

Independientemente de las aplicaciones instaladas en el servidor, cuando se inicien los servicios ser√° capaz de realizar el scraping.

La planificaci√≥n de su ejecuci√≥n en el servidor se puede hacer con airflow si se incluye en el contenedor o con crontab si no queremos incluir muchos componentes en el proyecto.

## Sobre los scrapers

He creado una clase base que tiene el c√≥digo para conectarse a Selenium en local o en remoto, si ejecutamos en local utiliza el DriverManager, en remoto utiliza la instalaci√≥n de Selenium en remoto.

* En local: pruebas en nuestros equipos
* En remoto (docker): Para despligue en servidor

## Mantenimiento de dependencias

Es importante para el correcto despliegue que el fichero `requirements.txt` tenga todas las dependencias bien definidas.

El proyecto tiene un entorno virtual, para crear el entorno y activarlo:
```
python -m venv .venv/
source .venv/bin/activate
pip install -r requirements.txt
```

Para a√±adir nuevas dependencias:
```
python -m venv .venv/
source .venv/bin/activate
pip install <nueva libreria>
pip freeze > requirements.txt
```

